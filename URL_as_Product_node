using Revise
using Mill, HTTP, JLD2, Random, OneHotArrays, Flux, Statistics, FindSteadyStates

include("eval_mod.jl")
using  .eval_mod



function main(epochs::Int64, batchsize::Int64, path_neur_1::Int64, path_neur_2::Int64, query_neur_1::Int64, query_neur_2::Int64, path_query_neur::Int64, 
    activs::Vector{String},   #act_path_neur_1::String, act_path_neur_2::String, act_query_neur_1::String, act_query_neur_2::String, act_path_query_neur::String, 
    percentage_of_data::Float64 = 5.0, lr::Float64 = 0.001, prime::Int64 = 2053, ngrams::Int64 = 3, base::Int64=256)::Nothing

    # Check for correct activation functions
    check_activation_func.(activs)

    # Read lines
    all_urls = JLD2.load("Julia/Lines/new_combined_lines.jld2", "lines")     #  Load URL addresses
    all_labels = JLD2.load("Julia/Lines/shuffled_labels.jld2", "labels_vector")      # Load labels for URLs

    # Generate random number every time
    random_seed = rand(UInt32)
    println("Seed: ", random_seed)

    new_indexing = randcycle(MersenneTwister(random_seed), length(all_urls))     # Generate permutation of set length, seed the generator with random_seed
    amount_of_training_data = Int(round((1/percentage_of_data)*length(new_indexing)))     # Specify the amount of data wanted

    # Create the training data and the evaluation data 
    training_lines = all_urls[new_indexing[1:amount_of_training_data]]      # Choose training lines
    training_labels = all_labels[new_indexing[1:amount_of_training_data]]     # Choose training labels

    eval_lines = all_urls[new_indexing[amount_of_training_data:end]]      # Choose eval lines
    eval_labels = all_labels[new_indexing[amount_of_training_data:end]]     # Choose eval labels

    # Calculate the percentage of positive instances within the data
    per_of_positive_instances = round((sum(training_labels))/length(training_labels), digits = 2)
    println("Percentage of positive urls within the training lines: ", per_of_positive_instances)
    println("Percentage of positive urls within the eval lines: ", sum(eval_labels)/length(eval_labels))
    println("Number of positive urls within eval data: ", sum(eval_labels))

    # Prepare training and evaluation data
    training_urls = url_to_mill.(training_lines, ngrams, prime, base)
    eval_urls = url_to_mill.(eval_lines, ngrams, prime, base)

    # Construct struct of parameters
    network_param = Network_params(random_seed, epochs, batchsize, prime, per_of_positive_instances, path_neur_1, path_neur_2, query_neur_1, query_neur_2, path_query_neur, 
     round(100/percentage_of_data, digits = 2), lr, activs...,)

    # Train model and get results on eval data
    predicted_labels, params_of_model = train(training_urls, eval_urls, training_labels, eval_labels, network_param)

    # Save results to a file
    eval_mod.eval_model("Julia/Model_eval.txt", eval_labels .+ 1, predicted_labels, network_param)

    return nothing
end

struct Network_params
    seed::Int64
    epochs::Int64
    batchsize::Int64
    prime::Int64
    positive_instances::Float64
    path_neur_1::Int64
    path_neur_2::Int64
    query_neur_1::Int64
    query_neur_2::Int64
    path_query_neur::Int64
    percentage_of_data::Float64
    learning_rate::Float64
    act_path_neur_1::String
    act_path_neur_2::String
    act_query_neur_1::String
    act_query_neur_2::String
    act_path_query_neur::String
end

# Struct that holds training parameters for grid search
mutable struct Training_params
    epochs::Float64
    batchsize::Float64
    path_neur_1::Float64
    path_neur_2::Float64
    query_neur_1::Float64
    query_neur_2::Float64
    learning_rate::Float64
end
# Outer constructor
function Training_params(vec::Vector{Float64})
    length(vec) == 7 || error("Input vector must have exactly 7 elements")
    Training_params(vec...) #splatt the vector
end

struct Additional_params
    seed::Int64
    prime::Int64
    percentage_of_data::Float64
end


# Takes in a URL as string and divides it, then creates bagnodes and finally a single product node for single URL
function url_to_mill(input::String, ngrams::Int64, prime::Int64, base::Int64)::ProductNode

    url = HTTP.URI(input) # Create "URL" type
    
    # Create parts of the URL
    host = url.host
    path = url.path
    query = url.query

    path = path[2:end] # Remove the first 

    # Prepare input into functions
    host = String.(split(host,"."))
    path = filter(!isempty, String.(split(path,"/")))
    query = String.(split(query,"&"))

    # Will not be including host into the product node, since it is the same for all url withing the training set
    model = Mill.ProductNode((transform(path, ngrams, prime, base), 
                             transform(query, ngrams, prime, base)
                                ))
    return model
end

# Transforms vector string[URL] into a BagNode
function transform(input::Vector{String}, ngrams::Int64, prime::Int64, base::Int64)::BagNode
    # Check for empty path
    if isempty(input)
        input = [""]
    end
    matrix = Mill.NGramMatrix(input, ngrams, base, prime)    # Create NGramMatrix from the string
    bn = Mill.BagNode(Mill.ArrayNode(matrix), [1:length(input)])     # NgramMatrix can act as ArrayNode data
    return bn
end

# Check if the present activation functions can be evaluated from the string - if they even exist
function check_activation_func(str::String)::Nothing
    funcs = ["relu", "sigmoid", "tanh", "elu"]
    in(str, funcs) ? true : error("Incorrect activation function!")
    return nothing
end

function create_model(param::Network_params)::ProductModel
    mod = Mill.ProductModel(tuple(
        Mill.BagModel(Mill.ArrayModel(Flux.Dense(param.prime => param.path_neur_1, bias = true, eval(Symbol(param.act_path_neur_1)))), 
                      Mill.SegmentedMeanMax(param.path_neur_1), Flux.Dense(2*param.path_neur_1 => param.path_neur_2, bias = true, eval(Symbol(param.act_path_neur_2)))),
        Mill.BagModel(Mill.ArrayModel(Flux.Dense(param.prime => param.query_neur_1, bias = true, eval(Symbol(param.act_query_neur_1)))), 
                      Mill.SegmentedMeanMax(param.query_neur_1), Flux.Dense(2*param.query_neur_1=> param.query_neur_2, bias = true, eval(Symbol(param.act_query_neur_2))))
                            ), Flux.Chain(Flux.Dense(param.path_neur_2 + param.query_neur_2 => param.path_query_neur, bias = true, eval(Symbol(param.act_path_query_neur))), Flux.Dense(param.path_query_neur=>2))
                            )
    return mod
end

# Basic train function ~ old
function train(training_urls, eval_urls, training_labels::Vector{Int64}, eval_labels::Vector{Int64}, param::Network_params)::Tuple
    
    # Create model of the network
    model = create_model(param)
    #printtree(model)

    # Create a loss
    loss(ds, y) = Flux.Losses.logitbinarycrossentropy(model(ds), OneHotArrays.onehotbatch(y .+ 1, 1:2))  # Onehot inside from training labels

    # Accuracy function
    acc(ds, y) = Statistics.mean(Flux.onecold(model(ds)) .== y.+ 1)    # Reverse of Onehot back to pure labels from the output matrix and calculating the mean of correctly predicted labels

    # Optimizer
    opt = Flux.Optimiser(ClipValue(1e-3), Adam(1e-3))
    par = Flux.params(model)    # Parameters of the model

    # Create minibatches
    training_data_loader = Flux.DataLoader((training_urls, training_labels), batchsize = param.batchsize, shuffle = true, partial = false)
    eval_data_loader = Flux.DataLoader((eval_urls, eval_labels), batchsize = param.batchsize, shuffle = true, partial = false)

    # Early stopping
    # Stopping  criteria for training loop
    state = Dict("best_loss" => Inf, "no_improv" => 0, "patience"=> 2)

    # Early stopping callback
    function early_stop_cb()
        eval_loss = mean(loss(batch...) for batch in eval_data_loader)
        println("procceded")
        if eval_loss < state["best_loss"]
            state["best_loss"] = eval_loss
            state["no_improv"] = 0
        else
            state["no_improv"] += 1
        end
        return state["no_improv"] > state["patience"]
    end
    
    # Callback
    #early_stopping_cb = Flux.throttle(() -> Flux.early_stopping(early_stop_cb, state["patience"])(), 1)

    # Training loop
    for i in 1:param.epochs
        Flux.Optimise.train!(loss, par, training_data_loader, opt)#, cb= early_stopping_cb) # That shouldn't be
        @show acc(training_urls, training_labels)
        # if i % 10 == 1
        #     @show acc(training_urls, training_labels)
        #     @show loss(training_urls, training_labels)
        # end
        if early_stop_cb()
            println("Stop at epoch: $i")
            break
        end
    end

    @show acc(eval_urls, eval_labels)   # Show accuracy of the model
    probs = softmax(model(eval_urls))   # Transform output into probabilities
    o = Flux.onecold(probs)             # Transform back the probabilities into labels
    # mn = mean(o .== eval_labels .+ 1)   # Calculate the mean of correct predictions
    # println("Mean of correct predictions: ", mn)

    return o, par
end

function train_grid(training_urls, eval_urls, training_labels::Vector{Int64}, eval_labels::Vector{Int64}, param::Training_params)::Tuple
    
    # Create model of the network
    model = create_model(param) # What with the activation functions
    #printtree(model)

    # Create a loss
    loss(ds, y) = Flux.Losses.logitbinarycrossentropy(model(ds), OneHotArrays.onehotbatch(y .+ 1, 1:2))  # Onehot inside from training labels

    # Accuracy function
    acc(ds, y) = Statistics.mean(Flux.onecold(model(ds)) .== y.+ 1)    # Reverse of Onehot back to pure labels from the output matrix and calculating the mean of correctly predicted labels

    # Optimizer
    opt = Flux.Optimiser(ClipValue(1e-3), Adam(1e-3))
    par = Flux.params(model)    # Parameters of the model

    # Create minibatches
    training_data_loader = Flux.DataLoader((training_urls, training_labels), batchsize = param.batchsize, shuffle = true, partial = false)
    eval_data_loader = Flux.DataLoader((eval_urls, eval_labels), batchsize = param.batchsize, shuffle = true, partial = false)

    # Early stopping
    # Stopping  criteria for training loop
    state = Dict("best_loss" => Inf, "no_improv" => 0, "patience"=> 2)

    # Early stopping callback
    function early_stop_cb()
        eval_loss = mean(loss(batch...) for batch in eval_data_loader)
        if eval_loss < state["best_loss"]
            state["best_loss"] = eval_loss
            state["no_improv"] = 0
        else
            state["no_improv"] += 1
        end
        return state["no_improv"] > state["patience"]
    end
    
    # Callback
    early_stopping_cb = Flux.throttle(() -> Flux.early_stopping(early_stop_cb, patience)(), 1)

    # Training loop
    for i in 1:param.epochs
        Flux.Optimise.train!(loss, par, training_data_loader, opt, cb = early_stopping_cb) # That shouldn't be
        # if i % 10 == 1
        #     @show acc(training_urls, training_labels)
        #     @show loss(training_urls, training_labels)
        # end
        if early_stop_cb()
            println("Stop at epoch: $i")
            break
        end
    end

    #@show acc(eval_urls, eval_labels)   # Show accuracy of the model
    probs = softmax(model(eval_urls))   # Transform output into probabilities
    o = Flux.onecold(probs)             # Transform back the probabilities into labels
    # mn = mean(o .== eval_labels .+ 1)   # Calculate the mean of correct predictions
    # println("Mean of correct predictions: ", mn)

    return o, par
end

"""
Create a CSV file ~ like a database -> use DataFrame structure, or SQL?
Save individual outputs of grid_search into separate files


"""

function grid_search()
    activs = ["elu", "relu", "relu", "relu", "relu"] 
    act = [Flux.activations.relu]
    # Create an order of activation functions to try, then create a specific range from numbers 1-#of activation functions
    
    parameters = ParameterGrid([[1,10,5], [20,30,5], [20,30,5], [20,30,5], [20,30,5], [20,30,5], [20,30,5]])
    structs = Training_params.(parameters)
    add_params = Additional_params(1,1,1.0)
    # USE pmap with Distributed
    #train(...)
    #permormance_val = eval_mod.evaluate_performance(true_labels, predicted_labels)
    #performances = pmap((x...)-> train


    # Save results into separate files using this
    #save("mod_params.jld2", "Network_params", "a", "Additional_params", "a", "Performance_metrics", "met", "model", "mod")

end
#grid_search()


activs = ["relu", "relu", "relu", "relu", "relu"]
# Epochs, batchsize, path_neur1, path_neur_2, query_neur_1, query_neur_2, percentage, learning rate = 0.001,prime = 2053, ngrams = 3, base = 256
main(40, 32, 16, 16, 256, 128, 64, activs, 10/8, 0.001)

# acc = [0.7, 0.8, 0.75, 0.9, 0.7]
# vss = [7, 16, 32, 64, 128]

# eval_mod.plot_acc_vs(acc, vss)
